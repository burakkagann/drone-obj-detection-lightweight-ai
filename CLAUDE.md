# CLAUDE.md - Project Context and Instructions

## Project Overview
**Master's Thesis Project**: "Robust Object Detection for Surveillance Drones in Low-Visibility Environments Using Lightweight AI Models"

**Student**: Burak KaÄŸan YÄ±lmazer  
**Program**: Computer Science - Big Data & Artificial Intelligence  
**Supervisors**: Prof. Dr. Alexander Iliev, Prof. Dr. Reiner Creutzburg  
**Timeline**: 40 days remaining (as of current date)  
**Due Date**: September 15, 2025

## Research Objectives
The primary goal is to develop lightweight object detection models capable of:
1. **Real-time performance** on edge devices (NVIDIA Jetson Nano, Raspberry Pi)
2. **Robust detection** in low-visibility environments (fog, nighttime, sensor distortions)
3. **Optimal trade-offs** between accuracy, speed, and model size for drone deployment

## Repository Structure and Key Components

### Core Models Under Investigation
1. **YOLOv5n** - Primary focus, proven baseline (22.6% mAP@0.5 on VisDrone)
2. **YOLOv8n** - Secondary comparison model
3. **MobileNet-SSD** - Lightweight alternative architecture
4. **NanoDet** - Ultra-lightweight edge-optimized model

### Datasets
- **VisDrone** - Primary dataset (10 classes, drone surveillance imagery)
- **DOTA** - Secondary dataset (15 classes, aerial imagery with OBB support)
- **CIFAR** - Under consideration (discuss with supervisor)

### Current Status
- **Phase**: Multi-model comparison framework implementation
- **YOLOv5n Best Performance**: Trial-2 achieving 23.557% mAP@0.5 (proven baseline)
- **YOLOv8n Status**: Baseline and Trial-1 framework COMPLETED âœ…
- **Current Priority**: Execute YOLOv8n training, then MobileNet-SSD, NanoDet
- **Next Steps**: Complete YOLOv8n evaluation, comparative analysis across all models

## Directory Structure

### Source Code (`src/`)
- `models/` - Model implementations and architectures
- `scripts/` - Training scripts organized by dataset
- `augmentation_pipeline/` - Environmental condition simulation
- `data_preparation/` - Dataset processing utilities
- `evaluation/` - Comprehensive metrics framework
- `utils/` - Shared utilities and helper functions

### Configuration (`config/`)
- Model-specific YAML configurations
- Dataset format definitions
- Hyperparameter optimization settings
- Augmentation pipeline parameters

### Data (`data/`)
- `raw/` - Original datasets
- `augmented/` - Environmentally augmented datasets
- `my_dataset/` - Processed and formatted datasets

### Results and Logs
- `runs/` - Training runs and TensorBoard logs
- `logs/` - Detailed training logs
- `checkpoints/` - Model weights and validation results
- `results/` - Performance analysis and thesis metrics

## Current Training Approach

### YOLOv5n Training Trials
- **Trial-1**: 100 epochs baseline (completed)
- **Trial-2**: Hyperparameter optimization (23.557% mAP@0.5 - PROVEN BASELINE) âœ…
- **Trial-3**: Enhanced optimization - CRITICAL FAILURE (0.002% mAP@0.5) âŒ

**CRITICAL FAILURE ANALYSIS**: Trial-3 resulted in 99.99% performance degradation due to:
- **Primary Cause**: Focal loss activation (`fl_gamma: 0.5`) when Trial-2 had it disabled (`fl_gamma: 0.0`)
- **Secondary Factors**: Over-regularization, loss weight imbalance, reduced learning rate
- **Status**: Root cause identified, recovery protocol established
- **Next Steps**: Return to Trial-2 baseline, implement incremental improvements only

**Research Impact**: Failure analysis demonstrates critical importance of controlled hyperparameter modification and provides valuable thesis methodology insights.

### Synthetic Augmentation Pipeline
Located in `src/augmentation_pipeline/`:
- **Fog simulation** (`fog.py`) - Atmospheric fog with depth blending
- **Night conditions** (`night.py`) - Low-light with gamma correction
- **Sensor distortions** (`sensor_distortions.py`) - Blur, noise, chromatic aberration
- **Quality validation** - SSIM/PSNR metrics for augmentation realism

## Performance Targets and Thresholds

### Recommended Performance Thresholds
- **Minimum Acceptable mAP@0.5**: 25%
- **Target mAP@0.5**: 30-35%
- **Real-time Performance**: >10 FPS
- **Model Size**: <10MB for edge deployment
- **Power Consumption**: <5W for drone applications

### Evaluation Priority (for thesis impact)
1. **Model Size vs. Accuracy Trade-off** (highest priority)
2. **Real-time Performance** (essential for surveillance)
3. **Robustness under Adverse Conditions** (unique contribution)
4. **Power Consumption** (important but secondary)

## Edge Device Testing Strategy

### Simulation-First Approach (Recommended for timeline)
- **NVIDIA Docker containers** with resource constraints
- **Google Colab** with limited resources
- **AWS EC2 instances** for Raspberry Pi simulation

### Hardware Testing (if budget permits)
- **Jetson Nano**: ~$99 (optimal for research validation)
- **Raspberry Pi 4**: ~$75 (broader applicability)
- **Timeline**: Order by Day 7 for Week 3 testing

## Environment Management

### Virtual Environment Requirements (CRITICAL)
**âš ï¸ MANDATORY**: Virtual environment MUST be activated before running ANY training scripts or model operations.

### All Available Virtual Environments

#### Primary Model Environments
1. **YOLOv5n Environment (PRIMARY)**
   - **Location**: `.\venvs\yolov5n_env\`
   - **Activation**: `.\venvs\yolov5n_env\Scripts\Activate.ps1`
   - **Used For**: All YOLOv5n training, evaluation, and related tasks

2. **MobileNet-SSD Environment**
   - **Location**: `.\venvs\mobilenet_ssd_env\`
   - **Activation**: `.\venvs\mobilenet_ssd_env\Scripts\Activate.ps1`
   - **Used For**: MobileNet-SSD training and evaluation

3. **NanoDet Environment**
   - **Location**: `.\venvs\nanodet_env\`
   - **Activation**: `.\venvs\nanodet_env\Scripts\Activate.ps1`
   - **Used For**: NanoDet training and evaluation

4. **TensorFlow Environment**
   - **Location**: `.\venvs\tensorflow_env\`
   - **Activation**: `.\venvs\tensorflow_env\Scripts\Activate.ps1`
   - **Used For**: TensorFlow-based models and operations

5. **Augmentation Environment**
   - **Location**: `.\venvs\augment_venv\`
   - **Activation**: `.\venvs\augment_venv\Scripts\Activate.ps1`
   - **Used For**: Synthetic data augmentation pipeline

#### Dataset-Specific Environments

6. **VisDrone YOLOv5n Environment**
   - **Location**: `.\venvs\visdrone\yolov5n_visdrone_env\`
   - **Activation**: `.\venvs\visdrone\yolov5n_visdrone_env\Scripts\Activate.ps1`
   - **Used For**: VisDrone-specific YOLOv5n training

7. **DOTA Model Environments**
   - **YOLOv5n DOTA**: `.\venvs\dota\venvs\yolov5n_dota_env\Scripts\Activate.ps1`
   - **YOLOv8n DOTA**: `.\venvs\dota\venvs\yolov8n_dota_env\Scripts\Activate.ps1`
   - **MobileNet-SSD DOTA**: `.\venvs\dota\venvs\mobilenet_ssd_dota_env\Scripts\Activate.ps1`
   - **NanoDet DOTA**: `.\venvs\dota\venvs\nanodet_dota_env\Scripts\Activate.ps1`

### ðŸ›¡ï¸ FRAMEWORK REPLICATION PROTOCOL FOR ALL FUTURE MODELS

**âš ï¸ CRITICAL: Use YOLOv8n framework as TEMPLATE for all future models**

**ðŸ”§ Directory Structure Template:**
```
src/scripts/[dataset]/[ModelName]/
â”œâ”€â”€ baseline/
â”‚   â”œâ”€â”€ train_[model]_baseline.py          # Phase 2: NO augmentation
â”‚   â””â”€â”€ run_[model]_baseline.ps1           # Professional wrapper
â”œâ”€â”€ trial-1/
â”‚   â”œâ”€â”€ train_[model]_trial1.py            # Phase 3: Synthetic augmentation
â”‚   â””â”€â”€ run_[model]_trial1.ps1             # Professional wrapper
â”œâ”€â”€ evaluation_metrics.py                  # Comprehensive evaluation
â””â”€â”€ README.md                              # Model-specific documentation
```

**ðŸ“‹ Baseline Script Requirements:**
- âŒ All augmentation disabled (hsv_h: 0.0, mosaic: 0.0, etc.)
- âœ… Original dataset only
- âœ… Comprehensive evaluation integration
- âœ… Methodology Section 4.1 metrics collection

**ðŸ“‹ Trial-1 Script Requirements:**
- âœ… Synthetic environmental augmentation
- âœ… Enhanced standard augmentation  
- âœ… Baseline comparison analysis
- âœ… Robustness evaluation

**ðŸ“‹ Evaluation Requirements:**
- âœ… All methodology metrics (mAP, FPS, memory, robustness)
- âœ… JSON export for analysis
- âœ… Markdown report generation
- âœ… Hardware performance measurement

### Virtual Environment Activation Protocol (MANDATORY)

**ðŸŽ¯ MODEL-SPECIFIC ENVIRONMENTS:**

```powershell
# Step 1: Navigate to repository root
cd "C:\Users\burak\OneDrive\Desktop\Git Repos\drone-obj-detection-lightweight-ai"

# Step 2: Activate model-specific environment
# YOLOv8n (COMPLETED):
.\venvs\yolov8n-visdrone_venv\Scripts\Activate.ps1

# YOLOv5n (PRIMARY - most common):
.\venvs\yolov5n_env\Scripts\Activate.ps1

# MobileNet-SSD (NEXT):
.\venvs\mobilenet_ssd_env\Scripts\Activate.ps1

# NanoDet (FUTURE):
.\venvs\nanodet_env\Scripts\Activate.ps1

# Step 3: Verify activation (should show environment name in prompt)
# Example: (yolov8n-visdrone_venv) PS C:\Users\burak\...

# Step 4: Execute training with methodology compliance
```

### Environment Selection Guidelines

**Use this environment for each task:**
- **YOLOv5n VisDrone Training**: `yolov5n_env` (PRIMARY)
- **MobileNet-SSD Training**: `mobilenet_ssd_env`
- **NanoDet Training**: `nanodet_env`
- **Augmentation Pipeline**: `augment_venv`
- **DOTA Dataset Training**: Use corresponding DOTA environment
- **TensorFlow Operations**: `tensorflow_env`

### Dependencies and Troubleshooting
- **Execution Policy**: `Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser`
- **Path Issues**: Always use absolute paths or navigate to repository root first
- **Verification**: Check for environment name prefix in terminal prompt after activation
- **Deactivation**: Use `deactivate` command when switching environments

## 40-Day Action Plan

### Week 1-2 (Days 1-14): Model Optimization & Core Results
1. Clean repository - delete Trial-3/4 folders
2. Optimize Trial-2 YOLOv5n baseline
3. Complete synthetic augmentation validation
4. Establish baseline performance metrics

### Week 3 (Days 15-21): Multi-Model Comparison
1. Train MobileNet-SSD on VisDrone
2. Train NanoDet on VisDrone
3. Comparative analysis framework
4. Edge device simulation setup

### Week 4-5 (Days 22-35): Edge Device Strategy & Analysis
1. Edge device performance testing
2. Results compilation and analysis
3. Begin thesis writing - methodology and results

### Week 6 (Days 36-40): Final Analysis & Submission
1. Final results visualization
2. Thesis completion and submission
3. Documentation finalization

## ðŸŽ¯ CRITICAL METHODOLOGY COMPLIANCE GUIDELINES

### âš ï¸ MANDATORY TRAINING APPROACH - NEVER DEVIATE FROM THIS

**ðŸ”´ PHASE 2: BASELINE TRAINING (Original Dataset Only)**
- **Purpose**: Establish TRUE performance benchmark
- **Requirements**: 
  - âŒ NO synthetic augmentation (fog, night, blur, rain)
  - âŒ NO standard augmentation (mosaic, mixup, HSV, geometric)
  - âœ… Original dataset images and labels ONLY
  - âœ… Minimal/disabled augmentation settings
- **Methodology Compliance**: Section 3.3 Phase 2

**ðŸŸ¢ PHASE 3: SYNTHETIC AUGMENTATION TRAINING**
- **Purpose**: Test synthetic augmentation impact vs baseline
- **Requirements**:
  - âœ… Synthetic environmental augmentation (fog, night, blur, rain)
  - âœ… Enhanced standard augmentation (mosaic, mixup, HSV, geometric)
  - âœ… Optimized hyperparameters for robustness
  - âœ… Baseline comparison analysis
- **Methodology Compliance**: Section 3.3 Phase 3

### ðŸ“Š MANDATORY EVALUATION METRICS (Section 4.1)
**Every training MUST collect:**
- âœ… **Detection Accuracy**: mAP@0.5, mAP@0.5:0.95, Precision, Recall, F1-Score
- âœ… **Inference Speed**: FPS, inference time (ms), speed analysis
- âœ… **Model Size**: File size (MB), memory usage (MB), parameter count
- âœ… **Robustness**: Performance degradation, baseline comparison
- âœ… **Hardware Info**: GPU, CUDA, memory specs

### ðŸ”„ COMPARATIVE ANALYSIS REQUIREMENTS (Section 4.2)
**For every model-dataset combination:**
1. **Baseline vs Augmented Comparison**: Quantified synthetic data impact
2. **Cross-Model Comparison**: Performance trade-offs analysis
3. **Edge Device Assessment**: Real-time performance evaluation
4. **Robustness Evaluation**: Environmental condition degradation

## Training Commands and Scripts

### YOLOv8n Training (COMPLETED âœ…)
```powershell
# Phase 2: Baseline (TRUE baseline - no augmentation)
.\src\scripts\visdrone\YOLOv8n\baseline\run_yolov8n_baseline.ps1

# Phase 3: Synthetic Augmentation + Optimization
.\src\scripts\visdrone\YOLOv8n\trial-1\run_yolov8n_trial1.ps1
```

### YOLOv5n Training
```bash
# Activate environment
# For YOLOv5n Trial-2 optimization
python src/models/YOLOv5/train.py --data config/visdrone/yolov5n_v1/yolov5n_visdrone_config.yaml --cfg src/models/YOLOv5/models/yolov5n.yaml --hyp config/visdrone/yolov5n_v1/hyp_visdrone_trial-2_optimized.yaml
```

### PowerShell Training Scripts - STANDARDIZED APPROACH

**MANDATORY**: Use Trial-2 wrapper script pattern for all future trials (most bulletproof):

#### Recommended Pattern (Trial-2 Style):
```powershell
# Create separate Python wrapper script for each trial
$pythonArgs = @(
    "train_yolov5n_trialX.py",
    "--epochs", $Epochs.ToString()
)

if ($QuickTest) {
    $pythonArgs += "--quick-test"
}

$process = Start-Process -FilePath "python" -ArgumentList $pythonArgs -NoNewWindow -PassThru -Wait
```

**Advantages:**
- âœ… No PowerShell path resolution issues
- âœ… Python handles all file paths internally  
- âœ… Proven reliability (Trial-2: 23.557% mAP@0.5)
- âœ… Simpler debugging and maintenance

#### Avoid Direct train.py Calls:
âŒ Complex relative paths (Trial-3 style)
âŒ Space-sensitive absolute paths  
âŒ PowerShell argument escaping issues

#### Warning Suppression for Clean Output:
```python
# Add to training scripts to suppress torch.cuda.amp.autocast warnings
import warnings
warnings.filterwarnings("ignore", message=".*torch.cuda.amp.autocast.*", category=FutureWarning)
```

#### Professional Logging Standards:
**CRITICAL: NO EMOJIS IN ANY CODE OR SCRIPTS**
- **Reason**: Windows terminal encoding cannot handle Unicode emojis properly
- **Use instead**: Professional text tags like [SUCCESS], [ERROR], [INFO], [WARNING]
- **Example**: 
  ```python
  # WRONG (causes encoding errors):
  logging.info("ðŸš€ Training started...")
  
  # CORRECT (professional and compatible):
  logging.info("[START] Training started...")
  ```

## Key Files and Configurations

### Critical Configuration Files
- `config/visdrone/yolov5n_v1/yolov5n_visdrone_config.yaml` - Dataset configuration
- `config/visdrone/yolov5n_v1/hyp_visdrone_trial-2_optimized.yaml` - Original Trial-2 hyperparameters
- `config/visdrone/yolov5n_v1/hyp_visdrone_trial3.yaml` - **CURRENT ACTIVE** Trial-3 hyperparameters
- `config/augmentation_config.yaml` - Augmentation pipeline settings

### Model Weights and Results
- `runs/train/yolov5n_trial2_BEST_23.557mAP/weights/best.pt` - **CURRENT BEST** Trial-2 model (23.557% mAP@0.5)
- `runs/train/yolov5n_trial2_BEST_23.557mAP/results.csv` - Current best training metrics

### Training Scripts (Trial-Based Organization)
- `src/scripts/visdrone/YOLOv5n/Trial-3/run_trial3_training.ps1` - **ACTIVE** Trial-3 training script

### Trial Organization Protocol (MANDATORY)
**General Practice**: Each trial gets its own folder under `src/scripts/visdrone/YOLOv5n/Trial-X/`

#### Trial Naming Convention:
- **Trial-1**: Baseline experiments
- **Trial-2**: First optimization iteration (23.557% mAP@0.5 achieved)
- **Trial-3**: Current optimization targeting 25%+ mAP@0.5
- **Trial-4**: Next iteration (if needed)
- **Trial-N**: Sequential numbering for all future trials

#### Folder Structure Requirements:
```
src/scripts/visdrone/YOLOv5n/
â”œâ”€â”€ Trial-1/          [Baseline scripts]
â”œâ”€â”€ Trial-2/          [First optimization scripts] 
â”œâ”€â”€ Trial-3/          [Current active scripts]
â””â”€â”€ Trial-X/          [Future trials...]
```

#### Configuration Naming:
- `config/visdrone/yolov5n_v1/hyp_visdrone_trial1.yaml`
- `config/visdrone/yolov5n_v1/hyp_visdrone_trial-2_optimized.yaml` 
- `config/visdrone/yolov5n_v1/hyp_visdrone_trial3.yaml`
- `config/visdrone/yolov5n_v1/hyp_visdrone_trial4.yaml` (future)

### Documentation
- `Z_Methodology.txt` - Complete thesis methodology and literature review
- `documentations/progression-notes/` - Detailed development history
- Various README files throughout the repository

## Common Issues and Solutions

### Training Issues
- **CUDA compatibility**: Ensure proper PyTorch CUDA version alignment
- **Dataset path issues**: Use absolute paths in configuration files
- **Memory issues**: Implement gradient checkpointing for large models

### Environment Issues
- **PowerShell execution policy**: `Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser`
- **Virtual environment conflicts**: Use separate environments for each model
- **Cross-platform compatibility**: Consider Docker containers for reproducibility

## ðŸŽ“ THESIS SUCCESS CRITERIA AND VALIDATION

### âœ… YOLOv8n Framework Validation (COMPLETED)
- **Methodology Compliance**: 100% alignment with thesis requirements
- **Phase 2 Baseline**: TRUE baseline (no augmentation) âœ…
- **Phase 3 Augmentation**: Synthetic + enhanced augmentation âœ…
- **Evaluation Metrics**: All Section 4.1 requirements âœ…
- **Comparative Analysis**: Baseline vs augmented quantification âœ…
- **Edge Performance**: FPS and memory measurement âœ…

### ðŸŽ¯ Success Thresholds for Thesis Impact
- **Minimum mAP@0.5**: 25% (methodology requirement)
- **Target mAP@0.5**: 30-35% (thesis significance)
- **Synthetic Augmentation Benefit**: >5% improvement over baseline
- **Real-time Performance**: >10 FPS on edge devices
- **Model Size**: <10MB for edge deployment

## Research Validation Approach

### Synthetic Augmentation Validation
- Simulation-based approach validated for thesis scope
- Comprehensive evaluation framework implemented: `evaluation_metrics.py` âœ…
- Baseline vs augmented quantified comparison âœ…
- Methodology Section 4.2 compliance achieved âœ…

### Performance Validation  
- Comprehensive metrics framework: `evaluation_metrics.py` âœ…
- Automated JSON export and Markdown reporting âœ…
- Hardware-aware performance measurement âœ…
- Multi-model comparative analysis framework âœ…

## Thesis Contribution Areas

### Primary Contributions
1. **Comprehensive benchmarking framework** for lightweight models on edge devices
2. **Synthetic augmentation pipeline** for adverse environmental conditions
3. **Performance trade-off analysis** between accuracy, speed, and robustness
4. **Practical deployment guidelines** for drone-based surveillance

### Academic Significance
- Addresses critical gap in edge-based drone surveillance
- Novel combination of lightweight models + environmental robustness
- Practical relevance for defense, search-and-rescue, and monitoring applications

## Notes for Claude Assistant

### Working Approach
- **Focus on immediate priorities**: 40-day deadline requires strategic focus
- **Proven baseline optimization**: Trial-2 (22.6% mAP) is the foundation to build upon
- **Practical solutions**: Simulation over hardware when time-constrained
- **Documentation-heavy**: Extensive documentation supports thesis writing

### Communication Style
- **Direct and actionable**: Provide specific commands and file paths
- **Timeline-aware**: Always consider 40-day constraint in recommendations
- **Research-focused**: Maintain academic rigor while being practical

### Repository Cleanup Priorities
1. ~~Delete Trial-3 and Trial-4 folders to reduce confusion~~ âœ… COMPLETED
2. Consolidate virtual environments where possible
3. Organize results and logs for thesis analysis
4. Maintain comprehensive documentation for reproducibility

### Documentation Management Protocol
**CRITICAL**: After every progression, all relevant information must be saved in `documentations/` directory structure:

#### Documentation Structure Requirements
- **Location**: `c:\Users\burak\OneDrive\Desktop\Git Repos\drone-obj-detection-lightweight-ai\documentations\`
- **Format**: All documentation must be in `.md` format
- **Naming Convention**: File names must align with their contents (e.g., `trial2_optimization_results.md`, `edge_device_simulation_setup.md`)
- **Auto-Creation**: If no relevant folder exists, create appropriate subdirectory structure

#### Mandatory Documentation Categories
Each category must be organized by **Model-Dataset** combinations (e.g., `YOLOv5n-Visdrone/`, `YOLOv8n-Visdrone/`, `MobileNet-SSD-Visdrone/`):

1. **`optimization-results/`** - All model optimization findings and results
   - Structure: `optimization-results/[Model-Dataset]/[trial_name]_[date].md`
   - Example: `optimization-results/YOLOv5n-Visdrone/trial2_enhanced_v2_optimization_20250121.md`

2. **`training-logs/`** - Detailed training session summaries and analysis
   - Structure: `training-logs/[Model-Dataset]/[session_name]_[date].md`
   - Example: `training-logs/YOLOv5n-Visdrone/trial2_enhanced_v2_training_20250121.md`

3. **`edge-device-testing/`** - Edge device simulation and testing documentation
   - Structure: `edge-device-testing/[Model-Dataset]/[device_type]_results_[date].md`
   - Example: `edge-device-testing/YOLOv5n-Visdrone/jetson_nano_simulation_20250121.md`

4. **`comparative-analysis/`** - Multi-model comparison results and insights
   - Structure: `comparative-analysis/[comparison_type]_[date].md`
   - Example: `comparative-analysis/yolov5n_vs_yolov8n_visdrone_20250121.md`

5. **`augmentation-validation/`** - Synthetic augmentation effectiveness analysis
   - Structure: `augmentation-validation/[Model-Dataset]/[augmentation_type]_validation_[date].md`
   - Example: `augmentation-validation/YOLOv5n-Visdrone/fog_augmentation_validation_20250121.md`

6. **`performance-benchmarks/`** - Comprehensive performance metrics and benchmarking
   - Structure: `performance-benchmarks/[Model-Dataset]/[benchmark_type]_[date].md`
   - Example: `performance-benchmarks/YOLOv5n-Visdrone/mAP_precision_recall_analysis_20250121.md`

7. **`thesis-analysis/`** - Analysis specifically for thesis writing and conclusions
   - Structure: `thesis-analysis/[analysis_topic]_[date].md`
   - Example: `thesis-analysis/lightweight_model_tradeoffs_20250121.md`

8. **`troubleshooting/`** - Issues encountered and solutions implemented
   - Structure: `troubleshooting/[Model-Dataset]/[issue_type]_[date].md`
   - Example: `troubleshooting/YOLOv5n-Visdrone/cuda_memory_issues_20250121.md`

9. **`methodology-updates/`** - Any changes or refinements to research methodology
   - Structure: `methodology-updates/[update_topic]_[date].md`
   - Example: `methodology-updates/hyperparameter_optimization_strategy_20250121.md`

#### Documentation Workflow
- **During Training**: Save training summaries with timestamp and configuration details
- **After Optimization**: Document hyperparameter changes and performance impacts
- **Post-Analysis**: Create comparative analysis reports with visualizations
- **Problem Resolution**: Document all issues and solutions for reproducibility
- **Milestone Completion**: Create comprehensive summary reports for each major milestone

#### File Naming Examples
- `trial2_hyperparameter_optimization_YYYYMMDD.md`
- `yolov5n_vs_mobilenet_ssd_comparison_YYYYMMDD.md`
- `edge_device_jetson_simulation_results_YYYYMMDD.md`
- `synthetic_augmentation_validation_metrics_YYYYMMDD.md`

---

## ðŸ§  CRITICAL MEMORY POINTS FOR CLAUDE ASSISTANT

### âš ï¸ NEVER FORGET THESE METHODOLOGY REQUIREMENTS:

1. **ðŸ”´ BASELINE = NO AUGMENTATION**: Phase 2 must have ALL augmentation disabled
2. **ðŸŸ¢ TRIAL-1 = SYNTHETIC AUGMENTATION**: Phase 3 must include environmental simulation
3. **ðŸ“Š COMPREHENSIVE METRICS**: Every training needs ALL Section 4.1 metrics
4. **ðŸ”„ BASELINE COMPARISON**: Trial-1 must compare against baseline results
5. **ðŸŽ¯ FRAMEWORK REPLICATION**: Use YOLOv8n structure for all future models

### ðŸŽ“ THESIS SUCCESS REQUIREMENTS:
- **Phase 2 vs Phase 3 comparison** (synthetic augmentation impact)
- **Multi-model comparative analysis** (YOLOv8n, YOLOv5n, MobileNet-SSD, NanoDet)
- **Edge device performance evaluation** (FPS, memory, model size)
- **Environmental robustness quantification** (fog, night, blur effects)

### ðŸ›¡ï¸ FRAMEWORK TEMPLATE FOR ALL MODELS:
```
src/scripts/[dataset]/[ModelName]/
â”œâ”€â”€ baseline/          # Phase 2: Original dataset only
â”œâ”€â”€ trial-1/           # Phase 3: Synthetic augmentation
â”œâ”€â”€ evaluation_metrics.py
â””â”€â”€ README.md
```

---

*Last Updated: January 2025*  
*This file serves as the primary context and methodology compliance guide for Claude assistant when working on this drone object detection research project.*